#!/usr/bin/env python3
"""
SWOT Analyzer CLI ‚Äî –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –≤ GitHub Actions
–° –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –Ω–∞—Ç–∏–≤–Ω–æ–≥–æ Anthropic Web Search
"""

import os
import sys
import json
import sqlite3
import hashlib
import argparse
from datetime import datetime
from pathlib import Path
from dataclasses import dataclass, field, asdict
from typing import List, Optional

import numpy as np
import anthropic

# LangChain
from langchain_anthropic import ChatAnthropic
from langchain_core.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate

# Embeddings
try:
    from sentence_transformers import SentenceTransformer
    EMBEDDINGS_AVAILABLE = True
except ImportError:
    EMBEDDINGS_AVAILABLE = False

# =============================================================================
# –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø
# =============================================================================

CLAUDE_MODEL = os.environ.get("CLAUDE_MODEL", "claude-sonnet-4-20250514")
SIMILARITY_THRESHOLD = 0.8
EMBEDDING_MODEL_NAME = "paraphrase-multilingual-MiniLM-L12-v2"


# =============================================================================
# DATA CLASSES
# =============================================================================

@dataclass
class SWOTItem:
    text: str
    reasoning: str
    embedding: Optional[List[float]] = None

@dataclass
class StrategicPair:
    factor1: str
    factor2: str
    strategy: str
    risk: Optional[str] = None

@dataclass
class SWOTAnalysis:
    source_file: str
    source_text: str
    context_hash: str
    strengths: List[SWOTItem] = field(default_factory=list)
    weaknesses: List[SWOTItem] = field(default_factory=list)
    opportunities: List[SWOTItem] = field(default_factory=list)
    threats: List[SWOTItem] = field(default_factory=list)
    strategic_so: List[StrategicPair] = field(default_factory=list)
    strategic_wo: List[StrategicPair] = field(default_factory=list)
    strategic_st: List[StrategicPair] = field(default_factory=list)
    strategic_wt: List[StrategicPair] = field(default_factory=list)
    created_at: Optional[str] = None

@dataclass
class ComparisonItem:
    old_text: Optional[str]
    new_text: Optional[str]
    change_type: str
    reasoning: str
    category: str

@dataclass
class SWOTComparison:
    old_id: int
    new_id: int
    items: List[ComparisonItem] = field(default_factory=list)
    summary: str = ""


# =============================================================================
# –ü–†–û–ú–ü–¢–´
# =============================================================================

SYSTEM_MESSAGE = """–¢—ã ‚Äî —ç–∫—Å–ø–µ—Ä—Ç –ø–æ —Å—Ç—Ä–∞—Ç–µ–≥–∏—á–µ—Å–∫–æ–º—É –∞–Ω–∞–ª–∏–∑—É –±–∏–∑–Ω–µ—Å–∞. 
–¢–≤–æ—è –∑–∞–¥–∞—á–∞ ‚Äî –ø—Ä–æ–≤–µ—Å—Ç–∏ –≥–ª—É–±–æ–∫–∏–π SWOT-–∞–Ω–∞–ª–∏–∑ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ.

–í–ê–ñ–ù–û:
- S –∏ W ‚Äî –í–ù–£–¢–†–ï–ù–ù–ò–ï —Ñ–∞–∫—Ç–æ—Ä—ã (—Ç–æ, —á—Ç–æ –∫–æ–º–ø–∞–Ω–∏—è –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ—Ç)
- O –∏ T ‚Äî –í–ù–ï–®–ù–ò–ï —Ñ–∞–∫—Ç–æ—Ä—ã (—Ä—ã–Ω–æ–∫, –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç—ã, —Ç—Ä–µ–Ω–¥—ã)
- –ö–∞–∂–¥—ã–π –ø—É–Ω–∫—Ç –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º –∏ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–º
- –û—Ç–≤–µ—á–∞–π –¢–û–õ–¨–ö–û –≤–∞–ª–∏–¥–Ω—ã–º JSON –±–µ–∑ markdown-–æ–±—ë—Ä—Ç–æ–∫"""

PROMPT_SW = """–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Ç–µ–∫—Å—Ç –∏ –≤—ã–¥–µ–ª–∏ –í–ù–£–¢–†–ï–ù–ù–ò–ï —Å–∏–ª—å–Ω—ã–µ –∏ —Å–ª–∞–±—ã–µ —Å—Ç–æ—Ä–æ–Ω—ã.

–ö–û–ù–¢–ï–ö–°–¢ –ö–û–ú–ü–ê–ù–ò–ò:
{context}

–¢–ï–ö–°–¢ –î–õ–Ø –ê–ù–ê–õ–ò–ó–ê:
{text}

–û—Ç–≤–µ—Ç—å —Å—Ç—Ä–æ–≥–æ –≤ JSON:
{{
    "strengths": [{{"text": "...", "reasoning": "..."}}],
    "weaknesses": [{{"text": "...", "reasoning": "..."}}]
}}

–ú–∏–Ω–∏–º—É–º 5 –ø—É–Ω–∫—Ç–æ–≤ –≤ –∫–∞–∂–¥–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏."""

PROMPT_OT_SEARCH = """–°—Ñ–æ—Ä–º—É–ª–∏—Ä—É–π 3-5 –ø–æ–∏—Å–∫–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –Ω–∞ —Ä—É—Å—Å–∫–æ–º –¥–ª—è –ø–æ–∏—Å–∫–∞ O –∏ T.

–ö–û–ù–¢–ï–ö–°–¢ –ö–û–ú–ü–ê–ù–ò–ò:
{context}

–û—Ç–≤–µ—Ç—å –≤ JSON:
{{
    "queries": ["–∑–∞–ø—Ä–æ—Å 1", "–∑–∞–ø—Ä–æ—Å 2", "–∑–∞–ø—Ä–æ—Å 3"]
}}"""

PROMPT_OT = """–í—ã–¥–µ–ª–∏ –í–ù–ï–®–ù–ò–ï –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –∏ —É–≥—Ä–æ–∑—ã.

–ö–û–ù–¢–ï–ö–°–¢ –ö–û–ú–ü–ê–ù–ò–ò:
{context}

–†–ï–ó–£–õ–¨–¢–ê–¢–´ –ò–°–°–õ–ï–î–û–í–ê–ù–ò–Ø –†–´–ù–ö–ê:
{search_results}

–û—Ç–≤–µ—Ç—å –≤ JSON:
{{
    "opportunities": [{{"text": "...", "reasoning": "..."}}],
    "threats": [{{"text": "...", "reasoning": "..."}}]
}}

–ú–∏–Ω–∏–º—É–º 5 –ø—É–Ω–∫—Ç–æ–≤ –≤ –∫–∞–∂–¥–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏."""

PROMPT_STRATEGIC = """–ü—Ä–æ–≤–µ–¥–∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏—á–µ—Å–∫–æ–µ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ SWOT.

STRENGTHS:
{strengths}

WEAKNESSES:
{weaknesses}

OPPORTUNITIES:
{opportunities}

THREATS:
{threats}

–û—Ç–≤–µ—Ç—å –≤ JSON:
{{
    "so": [{{"factor1": "–°–∏–ª–∞", "factor2": "–í–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å", "strategy": "..."}}],
    "wo": [{{"factor1": "–°–ª–∞–±–æ—Å—Ç—å", "factor2": "–í–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å", "strategy": "..."}}],
    "st": [{{"factor1": "–°–∏–ª–∞", "factor2": "–£–≥—Ä–æ–∑–∞", "strategy": "..."}}],
    "wt": [{{"factor1": "–°–ª–∞–±–æ—Å—Ç—å", "factor2": "–£–≥—Ä–æ–∑–∞", "strategy": "...", "risk": "..."}}]
}}

–ú–∏–Ω–∏–º—É–º 3 –ø–∞—Ä—ã –≤ –∫–∞–∂–¥–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏."""

PROMPT_COMPARISON = """–°—Ä–∞–≤–Ω–∏ –¥–≤–∞ SWOT-–∞–Ω–∞–ª–∏–∑–∞.

–ü–†–ï–î–´–î–£–©–ò–ô SWOT:
{old_swot}

–ù–û–í–´–ô SWOT:
{new_swot}

–ü–û–•–û–ñ–ò–ï –ü–ê–†–´ (similarity > 0.8):
{similar_pairs}

–û–ø—Ä–µ–¥–µ–ª–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∏–∑–º–µ–Ω–µ–Ω–∏—è:
- improved: —Å—Ç–∞–ª–æ –ª—É—á—à–µ
- worsened: —Å—Ç–∞–ª–æ —Ö—É–∂–µ  
- lost: –∏—Å—á–µ–∑–ª–æ (–æ–±—ä—è—Å–Ω–∏ –ø–æ—á–µ–º—É!)
- new: –Ω–æ–≤–æ–µ

–û—Ç–≤–µ—Ç—å –≤ JSON:
{{
    "items": [{{
        "old_text": "..." –∏–ª–∏ null,
        "new_text": "..." –∏–ª–∏ null,
        "change_type": "improved|worsened|lost|new",
        "reasoning": "–ü–æ–¥—Ä–æ–±–Ω–æ–µ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ",
        "category": "S|W|O|T"
    }}],
    "summary": "–ì–ª–∞–≤–Ω—ã–π –≤—ã–≤–æ–¥"
}}"""


# =============================================================================
# DATABASE
# =============================================================================

def init_db(db_path: Path) -> sqlite3.Connection:
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è SQLite"""
    conn = sqlite3.connect(str(db_path))
    conn.row_factory = sqlite3.Row
    cursor = conn.cursor()
    
    cursor.execute("""
        CREATE TABLE IF NOT EXISTS contexts (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            content TEXT NOT NULL,
            hash TEXT NOT NULL UNIQUE,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )
    """)
    
    cursor.execute("""
        CREATE TABLE IF NOT EXISTS swot_analyses (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            context_id INTEGER,
            source_file TEXT NOT NULL,
            source_text TEXT NOT NULL,
            strengths_json TEXT,
            weaknesses_json TEXT,
            opportunities_json TEXT,
            threats_json TEXT,
            strategic_so_json TEXT,
            strategic_wo_json TEXT,
            strategic_st_json TEXT,
            strategic_wt_json TEXT,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            FOREIGN KEY (context_id) REFERENCES contexts(id)
        )
    """)
    
    cursor.execute("""
        CREATE TABLE IF NOT EXISTS comparisons (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            old_swot_id INTEGER NOT NULL,
            new_swot_id INTEGER NOT NULL,
            items_json TEXT,
            summary TEXT,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )
    """)
    
    conn.commit()
    return conn


def get_or_create_context(conn: sqlite3.Connection, content: str) -> int:
    """–ü–æ–ª—É—á–∏—Ç—å –∏–ª–∏ —Å–æ–∑–¥–∞—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç"""
    content_hash = hashlib.sha256(content.encode()).hexdigest()[:16]
    cursor = conn.cursor()
    
    cursor.execute("SELECT id FROM contexts WHERE hash = ?", (content_hash,))
    row = cursor.fetchone()
    
    if row:
        return row[0]
    
    cursor.execute("INSERT INTO contexts (content, hash) VALUES (?, ?)", (content, content_hash))
    conn.commit()
    return cursor.lastrowid


def get_latest_swot(conn: sqlite3.Connection) -> Optional[dict]:
    """–ü–æ–ª—É—á–∏—Ç—å –ø–æ—Å–ª–µ–¥–Ω–∏–π SWOT"""
    cursor = conn.cursor()
    cursor.execute("SELECT * FROM swot_analyses ORDER BY created_at DESC LIMIT 1")
    row = cursor.fetchone()
    return dict(row) if row else None


def save_swot(conn: sqlite3.Connection, analysis: SWOTAnalysis, context_id: int) -> int:
    """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å SWOT"""
    def to_json(items):
        return json.dumps([asdict(i) for i in items], ensure_ascii=False)
    
    cursor = conn.cursor()
    cursor.execute("""
        INSERT INTO swot_analyses (
            context_id, source_file, source_text,
            strengths_json, weaknesses_json, opportunities_json, threats_json,
            strategic_so_json, strategic_wo_json, strategic_st_json, strategic_wt_json
        ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
    """, (
        context_id, analysis.source_file, analysis.source_text,
        to_json(analysis.strengths), to_json(analysis.weaknesses),
        to_json(analysis.opportunities), to_json(analysis.threats),
        to_json(analysis.strategic_so), to_json(analysis.strategic_wo),
        to_json(analysis.strategic_st), to_json(analysis.strategic_wt)
    ))
    conn.commit()
    return cursor.lastrowid


def load_swot_from_db(swot_dict: dict) -> SWOTAnalysis:
    """–ó–∞–≥—Ä—É–∑–∏—Ç—å SWOT –∏–∑ –ë–î"""
    def parse(json_str, cls):
        if not json_str:
            return []
        return [cls(**item) for item in json.loads(json_str)]
    
    return SWOTAnalysis(
        source_file=swot_dict['source_file'],
        source_text=swot_dict['source_text'],
        context_hash="",
        strengths=parse(swot_dict['strengths_json'], SWOTItem),
        weaknesses=parse(swot_dict['weaknesses_json'], SWOTItem),
        opportunities=parse(swot_dict['opportunities_json'], SWOTItem),
        threats=parse(swot_dict['threats_json'], SWOTItem),
        strategic_so=parse(swot_dict['strategic_so_json'], StrategicPair),
        strategic_wo=parse(swot_dict['strategic_wo_json'], StrategicPair),
        strategic_st=parse(swot_dict['strategic_st_json'], StrategicPair),
        strategic_wt=parse(swot_dict['strategic_wt_json'], StrategicPair),
        created_at=swot_dict['created_at']
    )


# =============================================================================
# LLM FUNCTIONS
# =============================================================================

def create_llm():
    """–°–æ–∑–¥–∞—Ç—å LLM –∫–ª–∏–µ–Ω—Ç –¥–ª—è LangChain"""
    api_key = os.environ.get("ANTHROPIC_API_KEY")
    if not api_key:
        raise ValueError("ANTHROPIC_API_KEY not set")
    
    return ChatAnthropic(
        model=CLAUDE_MODEL,
        max_tokens=4096,
        anthropic_api_key=api_key
    )


def create_search_client() -> anthropic.Anthropic:
    """–°–æ–∑–¥–∞—Ç—å –∫–ª–∏–µ–Ω—Ç Anthropic –¥–ª—è –≤–µ–±-–ø–æ–∏—Å–∫–∞"""
    api_key = os.environ.get("ANTHROPIC_API_KEY")
    if not api_key:
        raise ValueError("ANTHROPIC_API_KEY not set")
    return anthropic.Anthropic(api_key=api_key)


def parse_json_response(text: str) -> dict:
    """–ü–∞—Ä—Å–∏–Ω–≥ JSON –∏–∑ –æ—Ç–≤–µ—Ç–∞ —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫"""
    cleaned = text.strip()
    
    # –£–±–∏—Ä–∞–µ–º markdown-–æ–±—ë—Ä—Ç–∫–∏
    if cleaned.startswith("```json"):
        cleaned = cleaned[7:]
    if cleaned.startswith("```"):
        cleaned = cleaned[3:]
    if cleaned.endswith("```"):
        cleaned = cleaned[:-3]
    
    cleaned = cleaned.strip()
    
    try:
        return json.loads(cleaned)
    except json.JSONDecodeError as e:
        # –ü—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ JSON –≤ —Ç–µ–∫—Å—Ç–µ
        import re
        json_match = re.search(r'\{[\s\S]*\}', cleaned)
        if json_match:
            try:
                return json.loads(json_match.group())
            except json.JSONDecodeError:
                pass
        
        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON: {e}")
        print(f"   –¢–µ–∫—Å—Ç: {cleaned[:200]}...")
        raise


def invoke_llm(llm, prompt_template: str, variables: dict, max_retries: int = 2) -> dict:
    """–í—ã–∑–æ–≤ LLM —Å —Ä–µ—Ç—Ä–∞—è–º–∏"""
    prompt = ChatPromptTemplate.from_messages([
        SystemMessagePromptTemplate.from_template(SYSTEM_MESSAGE),
        HumanMessagePromptTemplate.from_template(prompt_template)
    ])
    
    chain = prompt | llm
    
    last_error = None
    for attempt in range(max_retries + 1):
        try:
            response = chain.invoke(variables)
            content = response.content if hasattr(response, 'content') else str(response)
            return parse_json_response(content)
        except json.JSONDecodeError as e:
            last_error = e
            if attempt < max_retries:
                print(f"   üîÑ Retry {attempt + 1}/{max_retries}...")
            continue
    
    raise last_error


def invoke_search(client: anthropic.Anthropic, query: str) -> str:
    """–í–µ–±-–ø–æ–∏—Å–∫ —á–µ—Ä–µ–∑ –Ω–∞—Ç–∏–≤–Ω—ã–π Anthropic API"""
    try:
        response = client.messages.create(
            model=CLAUDE_MODEL,
            max_tokens=4096,
            tools=[{
                "type": "web_search_20250305",
                "name": "web_search"
            }],
            messages=[{
                "role": "user", 
                "content": f"–ù–∞–π–¥–∏ –∞–∫—Ç—É–∞–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –ø–æ –∑–∞–ø—Ä–æ—Å—É –∏ –∫—Ä–∞—Ç–∫–æ –∏–∑–ª–æ–∂–∏ –∫–ª—é—á–µ–≤—ã–µ —Ñ–∞–∫—Ç—ã: {query}"
            }]
        )
        
        # –°–æ–±–∏—Ä–∞–µ–º —Ç–µ–∫—Å—Ç –∏–∑ –≤—Å–µ—Ö –±–ª–æ–∫–æ–≤
        result_parts = []
        for block in response.content:
            if hasattr(block, 'text'):
                result_parts.append(block.text)
        
        return "\n".join(result_parts) if result_parts else "–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã"
        
    except anthropic.APIError as e:
        print(f"   ‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞: {e}")
        return f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞: {str(e)}"


# =============================================================================
# EMBEDDINGS
# =============================================================================

def get_embedding_model():
    """–ó–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤"""
    if not EMBEDDINGS_AVAILABLE:
        print("‚ö†Ô∏è sentence-transformers –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω, —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –±—É–¥–µ—Ç –±–µ–∑ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤")
        return None
    return SentenceTransformer(EMBEDDING_MODEL_NAME)


def find_similar_pairs(old_texts: list, new_texts: list, model) -> list:
    """–ù–∞–π—Ç–∏ –ø–æ—Ö–æ–∂–∏–µ –ø–∞—Ä—ã"""
    if not model or not old_texts or not new_texts:
        return []
    
    old_emb = model.encode(old_texts)
    new_emb = model.encode(new_texts)
    
    pairs = []
    for i, old_txt in enumerate(old_texts):
        best_match = None
        best_score = 0.0
        
        for j, new_txt in enumerate(new_texts):
            score = float(np.dot(old_emb[i], new_emb[j]) / 
                         (np.linalg.norm(old_emb[i]) * np.linalg.norm(new_emb[j])))
            
            if score > best_score and score >= SIMILARITY_THRESHOLD:
                best_score = score
                best_match = (j, new_txt, score)
        
        if best_match:
            pairs.append({
                "old_text": old_txt,
                "new_text": best_match[1],
                "similarity": best_match[2]
            })
    
    return pairs


# =============================================================================
# MAIN ANALYSIS
# =============================================================================

def run_analysis(source_file: Path, context_file: Path, db_path: Path, outputs_dir: Path) -> tuple:
    """–ó–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞"""
    
    print(f"üìÑ –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º: {source_file}")
    
    # –ß–∏—Ç–∞–µ–º —Ñ–∞–π–ª—ã
    source_text = source_file.read_text(encoding='utf-8')
    context_text = context_file.read_text(encoding='utf-8') if context_file.exists() else ""
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
    conn = init_db(db_path)
    llm = create_llm()
    search_client = create_search_client()
    
    # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—ã–¥—É—â–∏–π SWOT
    prev_dict = get_latest_swot(conn)
    previous_swot = load_swot_from_db(prev_dict) if prev_dict else None
    
    print("üìä –ì–µ–Ω–µ—Ä–∞—Ü–∏—è S –∏ W...")
    sw_data = invoke_llm(llm, PROMPT_SW, {"context": context_text, "text": source_text})
    
    strengths = [SWOTItem(text=s["text"], reasoning=s["reasoning"]) for s in sw_data.get("strengths", [])]
    weaknesses = [SWOTItem(text=w["text"], reasoning=w["reasoning"]) for w in sw_data.get("weaknesses", [])]
    print(f"   ‚úÖ S: {len(strengths)}, W: {len(weaknesses)}")
    
    print("üîç –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–æ–∏—Å–∫–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤...")
    search_data = invoke_llm(llm, PROMPT_OT_SEARCH, {"context": context_text})
    queries = search_data.get("queries", ["—Ç—Ä–µ–Ω–¥—ã —Ä—ã–Ω–∫–∞"])
    
    print("üåê –í–µ–±-–ø–æ–∏—Å–∫...")
    search_results = []
    for q in queries[:3]:
        print(f"   üîé {q}")
        result = invoke_search(search_client, q)
        search_results.append(f"–ó–∞–ø—Ä–æ—Å: {q}\n–†–µ–∑—É–ª—å—Ç–∞—Ç: {result}\n")
    
    print("üìä –ì–µ–Ω–µ—Ä–∞—Ü–∏—è O –∏ T...")
    ot_data = invoke_llm(llm, PROMPT_OT, {"context": context_text, "search_results": "\n".join(search_results)})
    
    opportunities = [SWOTItem(text=o["text"], reasoning=o["reasoning"]) for o in ot_data.get("opportunities", [])]
    threats = [SWOTItem(text=t["text"], reasoning=t["reasoning"]) for t in ot_data.get("threats", [])]
    print(f"   ‚úÖ O: {len(opportunities)}, T: {len(threats)}")
    
    print("üéØ –°—Ç—Ä–∞—Ç–µ–≥–∏—á–µ—Å–∫–æ–µ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ...")
    strategic_data = invoke_llm(llm, PROMPT_STRATEGIC, {
        "strengths": "\n".join([f"- {s.text}" for s in strengths]),
        "weaknesses": "\n".join([f"- {w.text}" for w in weaknesses]),
        "opportunities": "\n".join([f"- {o.text}" for o in opportunities]),
        "threats": "\n".join([f"- {t.text}" for t in threats])
    })
    
    def parse_pairs(items, with_risk=False):
        return [StrategicPair(
            factor1=p.get("factor1", ""),
            factor2=p.get("factor2", ""),
            strategy=p.get("strategy", ""),
            risk=p.get("risk") if with_risk else None
        ) for p in items]
    
    strategic_so = parse_pairs(strategic_data.get("so", []))
    strategic_wo = parse_pairs(strategic_data.get("wo", []))
    strategic_st = parse_pairs(strategic_data.get("st", []))
    strategic_wt = parse_pairs(strategic_data.get("wt", []), with_risk=True)
    
    # –°–æ–∑–¥–∞—ë–º –æ–±—ä–µ–∫—Ç
    analysis = SWOTAnalysis(
        source_file=source_file.name,
        source_text=source_text,
        context_hash=hashlib.sha256(context_text.encode()).hexdigest()[:16],
        strengths=strengths,
        weaknesses=weaknesses,
        opportunities=opportunities,
        threats=threats,
        strategic_so=strategic_so,
        strategic_wo=strategic_wo,
        strategic_st=strategic_st,
        strategic_wt=strategic_wt
    )
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º
    context_id = get_or_create_context(conn, context_text)
    swot_id = save_swot(conn, analysis, context_id)
    print(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: ID={swot_id}")
    
    # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ
    comparison = None
    if previous_swot:
        print("üîÑ –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å –ø—Ä–µ–¥—ã–¥—É—â–∏–º...")
        
        embed_model = get_embedding_model()
        
        old_texts = ([s.text for s in previous_swot.strengths] +
                    [w.text for w in previous_swot.weaknesses] +
                    [o.text for o in previous_swot.opportunities] +
                    [t.text for t in previous_swot.threats])
        
        new_texts = ([s.text for s in strengths] +
                    [w.text for w in weaknesses] +
                    [o.text for o in opportunities] +
                    [t.text for t in threats])
        
        similar_pairs = find_similar_pairs(old_texts, new_texts, embed_model)
        
        comp_data = invoke_llm(llm, PROMPT_COMPARISON, {
            "old_swot": json.dumps({
                "strengths": [s.text for s in previous_swot.strengths],
                "weaknesses": [w.text for w in previous_swot.weaknesses],
                "opportunities": [o.text for o in previous_swot.opportunities],
                "threats": [t.text for t in previous_swot.threats]
            }, ensure_ascii=False),
            "new_swot": json.dumps({
                "strengths": [s.text for s in strengths],
                "weaknesses": [w.text for w in weaknesses],
                "opportunities": [o.text for o in opportunities],
                "threats": [t.text for t in threats]
            }, ensure_ascii=False),
            "similar_pairs": json.dumps(similar_pairs, ensure_ascii=False)
        })
        
        comparison = SWOTComparison(
            old_id=0, new_id=swot_id,
            items=[ComparisonItem(
                old_text=ci.get("old_text"),
                new_text=ci.get("new_text"),
                change_type=ci.get("change_type", ""),
                reasoning=ci.get("reasoning", ""),
                category=ci.get("category", "")
            ) for ci in comp_data.get("items", [])],
            summary=comp_data.get("summary", "")
        )
    
    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á—ë—Ç–æ–≤
    outputs_dir.mkdir(exist_ok=True)
    
    swot_md = generate_swot_markdown(analysis)
    swot_path = outputs_dir / "swot_latest.md"
    swot_path.write_text(swot_md, encoding='utf-8')
    print(f"üìÑ SWOT: {swot_path}")
    
    comparison_path = None
    if comparison:
        comp_md = generate_comparison_markdown(comparison)
        comparison_path = outputs_dir / "comparison_latest.md"
        comparison_path.write_text(comp_md, encoding='utf-8')
        print(f"üìÑ –°—Ä–∞–≤–Ω–µ–Ω–∏–µ: {comparison_path}")
    
    conn.close()
    print("‚úÖ –ì–æ—Ç–æ–≤–æ!")
    
    return analysis, comparison, swot_path, comparison_path


# =============================================================================
# MARKDOWN GENERATION
# =============================================================================

def generate_swot_markdown(analysis: SWOTAnalysis) -> str:
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è SWOT –≤ Markdown"""
    
    md = f"""# SWOT-–∞–Ω–∞–ª–∏–∑: {analysis.source_file}

**–î–∞—Ç–∞:** {datetime.now().strftime("%Y-%m-%d %H:%M")}

---

## Strengths (–°–∏–ª—å–Ω—ã–µ —Å—Ç–æ—Ä–æ–Ω—ã)

| ‚Ññ | –ü—É–Ω–∫—Ç | –û–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ |
|---|-------|-------------|
"""
    for idx, s in enumerate(analysis.strengths, 1):
        md += f"| {idx} | {s.text} | {s.reasoning} |\n"
    
    md += """
## Weaknesses (–°–ª–∞–±—ã–µ —Å—Ç–æ—Ä–æ–Ω—ã)

| ‚Ññ | –ü—É–Ω–∫—Ç | –û–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ |
|---|-------|-------------|
"""
    for idx, w in enumerate(analysis.weaknesses, 1):
        md += f"| {idx} | {w.text} | {w.reasoning} |\n"
    
    md += """
## Opportunities (–í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏)

| ‚Ññ | –ü—É–Ω–∫—Ç | –û–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ |
|---|-------|-------------|
"""
    for idx, o in enumerate(analysis.opportunities, 1):
        md += f"| {idx} | {o.text} | {o.reasoning} |\n"
    
    md += """
## Threats (–£–≥—Ä–æ–∑—ã)

| ‚Ññ | –ü—É–Ω–∫—Ç | –û–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ |
|---|-------|-------------|
"""
    for idx, t in enumerate(analysis.threats, 1):
        md += f"| {idx} | {t.text} | {t.reasoning} |\n"
    
    md += """
---

## –°—Ç—Ä–∞—Ç–µ–≥–∏—á–µ—Å–∫–æ–µ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ

### S+O (–ù–∞—Å—Ç—É–ø–∞—Ç–µ–ª—å–Ω–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è)

| –°–∏–ª–∞ | –í–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å | –°—Ç—Ä–∞—Ç–µ–≥–∏—è |
|------|-------------|-----------|
"""
    for pair in analysis.strategic_so:
        md += f"| {pair.factor1} | {pair.factor2} | {pair.strategy} |\n"
    
    md += """
### W+O (–°—Ç—Ä–∞—Ç–µ–≥–∏—è —É–ª—É—á—à–µ–Ω–∏–π)

| –°–ª–∞–±–æ—Å—Ç—å | –í–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å | –°—Ç—Ä–∞—Ç–µ–≥–∏—è |
|----------|-------------|-----------|
"""
    for pair in analysis.strategic_wo:
        md += f"| {pair.factor1} | {pair.factor2} | {pair.strategy} |\n"
    
    md += """
### S+T (–ó–∞—â–∏—Ç–Ω–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è)

| –°–∏–ª–∞ | –£–≥—Ä–æ–∑–∞ | –°—Ç—Ä–∞—Ç–µ–≥–∏—è |
|------|--------|-----------|
"""
    for pair in analysis.strategic_st:
        md += f"| {pair.factor1} | {pair.factor2} | {pair.strategy} |\n"
    
    md += """
### W+T (–ú–∏–Ω–∏–º–∏–∑–∞—Ü–∏—è —Ä–∏—Å–∫–æ–≤)

| –°–ª–∞–±–æ—Å—Ç—å | –£–≥—Ä–æ–∑–∞ | –†–∏—Å–∫ | –°—Ç—Ä–∞—Ç–µ–≥–∏—è |
|----------|--------|------|-----------|
"""
    for pair in analysis.strategic_wt:
        md += f"| {pair.factor1} | {pair.factor2} | {pair.risk or '-'} | {pair.strategy} |\n"
    
    return md


def generate_comparison_markdown(comparison: SWOTComparison) -> str:
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –≤ Markdown"""
    
    md = f"""# –°—Ä–∞–≤–Ω–µ–Ω–∏–µ SWOT-–∞–Ω–∞–ª–∏–∑–æ–≤

**–î–∞—Ç–∞:** {datetime.now().strftime("%Y-%m-%d %H:%M")}

---

## –ì–ª–∞–≤–Ω—ã–π –≤—ã–≤–æ–¥

{comparison.summary}

---

## –î–µ—Ç–∞–ª–∏ –∏–∑–º–µ–Ω–µ–Ω–∏–π

"""
    for change_type, emoji, title in [
        ("improved", "‚úÖ", "–£–ª—É—á—à–∏–ª–æ—Å—å"),
        ("new", "üÜï", "–ù–æ–≤–æ–µ"),
        ("worsened", "‚ö†Ô∏è", "–£—Ö—É–¥—à–∏–ª–æ—Å—å"),
        ("lost", "‚ùå", "–ü–æ—Ç–µ—Ä—è–Ω–æ")
    ]:
        filtered = [item for item in comparison.items if item.change_type == change_type]
        if filtered:
            md += f"### {emoji} {title}\n\n"
            md += "| –ö–∞—Ç–µ–≥–æ—Ä–∏—è | –ë—ã–ª–æ | –°—Ç–∞–ª–æ | –û–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ |\n"
            md += "|-----------|------|-------|-------------|\n"
            for item in filtered:
                md += f"| {item.category} | {item.old_text or '-'} | {item.new_text or '-'} | {item.reasoning} |\n"
            md += "\n"
    
    return md


def generate_pr_comment(analysis: SWOTAnalysis, comparison: Optional[SWOTComparison]) -> str:
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è –¥–ª—è PR"""
    
    comment = f"""## üéØ SWOT-–∞–Ω–∞–ª–∏–∑: `{analysis.source_file}`

### üìä –†–µ–∑—É–ª—å—Ç–∞—Ç

| –ö–∞—Ç–µ–≥–æ—Ä–∏—è | –ö–æ–ª-–≤–æ |
|-----------|--------|
| üí™ Strengths | {len(analysis.strengths)} |
| üò∞ Weaknesses | {len(analysis.weaknesses)} |
| üöÄ Opportunities | {len(analysis.opportunities)} |
| ‚ö†Ô∏è Threats | {len(analysis.threats)} |

<details>
<summary>üìã Strengths</summary>

"""
    for s in analysis.strengths:
        comment += f"- **{s.text}**\n"
    
    comment += """
</details>

<details>
<summary>üìã Weaknesses</summary>

"""
    for w in analysis.weaknesses:
        comment += f"- **{w.text}**\n"
    
    comment += """
</details>

<details>
<summary>üìã Opportunities</summary>

"""
    for o in analysis.opportunities:
        comment += f"- **{o.text}**\n"
    
    comment += """
</details>

<details>
<summary>üìã Threats</summary>

"""
    for t in analysis.threats:
        comment += f"- **{t.text}**\n"
    
    comment += """
</details>

"""
    
    if comparison and comparison.items:
        improved = len([c for c in comparison.items if c.change_type == "improved"])
        worsened = len([c for c in comparison.items if c.change_type == "worsened"])
        lost = len([c for c in comparison.items if c.change_type == "lost"])
        new_count = len([c for c in comparison.items if c.change_type == "new"])
        
        comment += f"""
---

### üîÑ –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å –ø—Ä–µ–¥—ã–¥—É—â–∏–º

| –ò–∑–º–µ–Ω–µ–Ω–∏–µ | –ö–æ–ª-–≤–æ |
|-----------|--------|
| ‚úÖ –£–ª—É—á—à–∏–ª–æ—Å—å | {improved} |
| üÜï –ù–æ–≤–æ–µ | {new_count} |
| ‚ö†Ô∏è –£—Ö—É–¥—à–∏–ª–æ—Å—å | {worsened} |
| ‚ùå –ü–æ—Ç–µ—Ä—è–Ω–æ | {lost} |

**–í—ã–≤–æ–¥:** {comparison.summary}
"""
    
    comment += """
---

üìÑ –ü–æ–ª–Ω—ã–π –æ—Ç—á—ë—Ç: `outputs/swot_latest.md`
"""
    
    return comment


# =============================================================================
# CLI
# =============================================================================

def main():
    parser = argparse.ArgumentParser(description="SWOT Analyzer CLI")
    parser.add_argument("source_file", type=Path, help="–ü—É—Ç—å –∫ .md —Ñ–∞–π–ª—É –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")
    parser.add_argument("--context", type=Path, default=Path("context.md"), help="–ü—É—Ç—å –∫ context.md")
    parser.add_argument("--db", type=Path, default=Path("swot.db"), help="–ü—É—Ç—å –∫ SQLite –±–∞–∑–µ")
    parser.add_argument("--outputs", type=Path, default=Path("outputs"), help="–ü–∞–ø–∫–∞ –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
    parser.add_argument("--comment-file", type=Path, help="–§–∞–π–ª –¥–ª—è –∑–∞–ø–∏—Å–∏ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è –∫ PR")
    
    args = parser.parse_args()
    
    if not args.source_file.exists():
        print(f"‚ùå –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {args.source_file}")
        sys.exit(1)
    
    try:
        analysis, comparison, swot_path, comp_path = run_analysis(
            args.source_file,
            args.context,
            args.db,
            args.outputs
        )
        
        if args.comment_file:
            comment = generate_pr_comment(analysis, comparison)
            args.comment_file.write_text(comment, encoding='utf-8')
            print(f"üí¨ –ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π: {args.comment_file}")
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
